/*
 * Copyright 2023-2023 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.logaritex.ai.api;

import java.util.List;
import java.util.Map;
import java.util.function.Consumer;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.logaritex.ai.api.ChatCompletionApi.ChatCompletion.Choice;
import com.logaritex.ai.api.ChatCompletionApi.ChatCompletion.Usage;
import com.logaritex.ai.api.ChatCompletionApi.ChatCompletionChunk.ChunkChoice;
import com.logaritex.ai.api.ChatCompletionApi.ChatCompletionRequest.ResponseFormat;
import com.logaritex.ai.api.ChatCompletionApi.ChatCompletionRequest.ToolChoice;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.util.Assert;
import org.springframework.web.client.ResponseErrorHandler;
import org.springframework.web.client.RestClient;
import org.springframework.web.reactive.function.client.WebClient;

/**
 * Single class implementation of the OpenAI Chat Completion API: https://beta.openai.com/docs/api-reference/chat
 *
 * @author Christian Tzolov
 */
public class ChatCompletionApi {

	private static final String DEFAULT_BASE_URL = "https://api.openai.com";

	private final Consumer<HttpHeaders> jsonContentHeaders;
	private final ResponseErrorHandler responseErrorHandler;
	private final RestClient restClient;
	private final WebClient webClient;
	private final ObjectMapper objectMapper;

	/**
	 * Create an new chat completion api.
	 *
	 * @param openAiToken OpenAI apiKey.
	 */
	public ChatCompletionApi(String openAiToken) {
		this(DEFAULT_BASE_URL, openAiToken, RestClient.builder());
	}

	/**
	 * Create an new chat completion api.
	 *
	 * @param baseUrl api base URL.
	 * @param openAiToken OpenAI apiKey.
	 * @param restClientBuilder RestClient builder.
	 */
	public ChatCompletionApi(String baseUrl, String openAiToken, RestClient.Builder restClientBuilder) {

		this.objectMapper = new ObjectMapper();
		this.jsonContentHeaders = headers -> {
			headers.setBearerAuth(openAiToken);
			headers.setContentType(MediaType.APPLICATION_JSON);
		};

		this.responseErrorHandler = new OpenAiResponseErrorHandler();

		this.restClient = restClientBuilder
				.baseUrl(baseUrl)
				.defaultHeaders(jsonContentHeaders)
				.build();

		this.webClient = WebClient.builder()
				.baseUrl(baseUrl)
				.defaultHeaders(jsonContentHeaders)
				.build();
	}

	/**
	 * Creates a model response for the given chat conversation.
	 *
	 * @param messages A list of messages comprising the conversation so far.
	 * @param model ID of the model to use.
	 * @param frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
	 * frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	 * @param logit_bias Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object
	 * that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
	 * Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will
	 * vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100
	 * or 100 should result in a ban or exclusive selection of the relevant token.
	 * @param max_tokens The maximum number of tokens to generate in the chat completion. The total length of input
	 * tokens and generated tokens is limited by the model's context length.
	 * @param n How many chat completion choices to generate for each input message. Note that you will be charged based
	 * on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
	 * @param presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
	 * appear in the text so far, increasing the model's likelihood to talk about new topics.
	 * @param response_format An object specifying the format that the model must output. Setting to { "type":
	 * "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.
	 * @param seed This feature is in Beta. If specified, our system will make a best effort to sample
	 * deterministically, such that repeated requests with the same seed and parameters should return the same result.
	 * Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor
	 * changes in the backend.
	 * @param stop Up to 4 sequences where the API will stop generating further tokens.
	 * @param stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
	 * server-sent events as they become available, with the stream terminated by a data: [DONE] message.
	 * @param temperature What sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output
	 * more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend
	 * altering this or top_p but not both.
	 * @param top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the
	 * results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10%
	 * probability mass are considered. We generally recommend altering this or temperature but not both.
	 * @param tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to
	 * provide a list of functions the model may generate JSON inputs for.
	 * @param tool_choice Controls which (if any) function is called by the model. none means the model will not call a
	 * function and instead generates a message. auto means the model can pick between generating a message or calling a
	 * function. Specifying a particular function via {"type: "function", "function": {"name": "my_function"}} forces
	 * the model to call that function. none is the default when no functions are present. auto is the default if
	 * functions are present.
	 * @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	 *
	 */
	@JsonInclude(Include.NON_NULL)
	public record ChatCompletionRequest(
			List<ChatCompletionMessage> messages, String model, Float frequency_penalty,
			Map<String, Object> logit_bias,
			Integer max_tokens, Integer n, Float presence_penalty, ResponseFormat response_format, Integer seed,
			String stop, Boolean stream, Float temperature, Float top_p, List<Data.FunctionTool> tools,
			ToolChoice tool_choice, String user) {

		public ChatCompletionRequest(List<ChatCompletionMessage> messages, String model) {
			this(messages, model, 0.0f, null, null, 1, 0.0f, null, null, null, false, 0.8f, null,
					null, null, null);
		}

		public ChatCompletionRequest(List<ChatCompletionMessage> messages, String model, boolean stream) {
			this(messages, model, 0.0f, null, null, 1, 0.0f, null, null, null, stream, 0.8f, null,
					null, null, null);
		}

		public ChatCompletionRequest(List<ChatCompletionMessage> messages, String model,
				List<Data.FunctionTool> tools, ToolChoice tool_choice) {
			this(messages, model, 0.0f, null, null, 1, 0.0f, null, null, null, false, 0.8f, null,
					tools, tool_choice, null);
		}

		/**
		 * Specifies a tool the model should use. Use to force the model to call a specific function.
		 *
		 * @param type The type of the tool. Currently, only 'function' is supported.
		 * @param function single field map for type 'name':'your function name'.
		 */
		public record ToolChoice(String type, Map<String, String> function) {

			public ToolChoice(String functionName) {
				this("function", Map.of("name", functionName));
			}

		}

		/**
		 * An object specifying the format that the model must output.
		 * @param type Must be one of 'text' or 'json_object'.
		 */
		public record ResponseFormat(String type) {

		}

	}

	/**
	 * Message comprising the conversation.
	 *
	 * @param content The contents of the message.
	 * @param role The role of the messages author. Could be one of the {@link Role} types.
	 * @param name An optional name for the participant. Provides the model information to differentiate between
	 * participants of the same role.
	 * @param tool_call_id Tool call that this message is responding to. Only applicable for the {@link Role#tool} role
	 * and null otherwise.
	 * @param tool_calls The tool calls generated by the model, such as function calls. Applicable only for
	 * {@link Role#assistant} role and null otherwise.
	 * @param function_call Deprecated and replaced by tool_calls. The name and arguments of a function that should be
	 * called, as generated by the model.
	 */
	@JsonInclude(Include.NON_NULL)
	public record ChatCompletionMessage(String content, Role role, String name, String tool_call_id,
			List<ToolCall> tool_calls, ChatCompletionFunction function_call) {

		public ChatCompletionMessage(String content, Role role) {
			this(content, role, null, null, null, null);
		}

		/**
		 * The role of the author of this message.
		 */
		public enum Role {
			/**
			 * System message.
			 */
			system,
			/**
			 * User message.
			 */
			user,
			/**
			 * Assistant message.
			 */
			assistant,
			/**
			 * Tool message.
			 */
			tool
		}

		/**
		 * The relevant tool call.
		 *
		 * @param id The ID of the tool call. This ID must be referenced when you submit the tool outputs in using the
		 * Submit tool outputs to run endpoint.
		 * @param type The type of tool call the output is required for. For now, this is always function.
		 * @param function The function definition.
		 */
		public record ToolCall(String id, String type, ChatCompletionFunction function) {

		}

		/**
		 * The function definition.
		 *
		 * @param name The name of the function.
		 * @param arguments The arguments that the model expects you to pass to the function.
		 */
		public record ChatCompletionFunction(String name, String arguments) {

		}
	}

	/**
	 * The reason the model stopped generating tokens.
	 */
	public enum ChatCompletionFinishReason {
		/**
		 * The model hit a natural stop point or a provided stop sequence.
		 */
		stop,
		/**
		 * The maximum number of tokens specified in the request was reached.
		 */
		length,
		/**
		 * The content was omitted due to a flag from our content filters.
		 */
		content_filter,
		/**
		 * The model called a tool.
		 */
		tool_calls,
		/**
		 * (deprecated) The model called a function.
		 */
		function_call
	}

	/**
	 * Represents a chat completion response returned by model, based on the provided input.
	 *
	 * @param id A unique identifier for the chat completion.
	 * @param choices A list of chat completion choices. Can be more than one if n is greater than 1.
	 * @param created The Unix timestamp (in seconds) of when the chat completion was created.
	 * @param model The model used for the chat completion.
	 * @param system_fingerprint This fingerprint represents the backend configuration that the model runs with. Can be
	 * used in conjunction with the seed request parameter to understand when backend changes have been made that might
	 * impact determinism.
	 * @param object The object type, which is always chat.completion.
	 * @param usage Usage statistics for the completion request.
	 */
	public record ChatCompletion(String id, List<Choice> choices, Long created, String model,
			String system_fingerprint, String object, Usage usage) {

		/**
		 * Chat completion choice.
		 *
		 * @param finish_reason The reason the model stopped generating tokens.
		 * @param index The index of the choice in the list of choices.
		 * @param message A chat completion message generated by the model.
		 */
		public record Choice(ChatCompletionFinishReason finish_reason, Integer index, ChatCompletionMessage message) {

		}

		/**
		 * Usage statistics for the completion request.
		 *
		 * @param completion_tokens Number of tokens in the generated completion.
		 * @param prompt_tokens Number of tokens in the prompt.
		 * @param total_tokens Total number of tokens used in the request (prompt + completion).
		 */
		public record Usage(Integer completion_tokens, Integer prompt_tokens, Integer total_tokens) {

		}
	}

	/**
	 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
	 *
	 * @param id A unique identifier for the chat completion. Each chunk has the same ID.
	 * @param choices A list of chat completion choices. Can be more than one if n is greater than 1.
	 * @param created The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same
	 * timestamp.
	 * @param model The model used for the chat completion.
	 * @param system_fingerprint This fingerprint represents the backend configuration that the model runs with. Can be
	 * used in conjunction with the seed request parameter to understand when backend changes have been made that might
	 * impact determinism.
	 * @param object The object type, which is always 'chat.completion.chunk'.
	 */
	public record ChatCompletionChunk(String id, List<ChunkChoice> choices, Long created, String model,
			String system_fingerprint, String object) {

		/**
		 * Chat completion choice.
		 *
		 * @param finish_reason The reason the model stopped generating tokens.
		 * @param index The index of the choice in the list of choices.
		 * @param delta A chat completion delta generated by streamed model responses.
		 */
		@JsonInclude(Include.NON_NULL)
		public record ChunkChoice(ChatCompletionFinishReason finish_reason, Integer index,
				ChatCompletionMessage delta) {

		}
	}

	/**
	 * Creates a model response for the given chat conversation.
	 * @param chatRequest The chat completion request.
	 * @return The chat completion response.
	 */
	public ChatCompletion chatCompletion(ChatCompletionRequest chatRequest) {

		Assert.notNull(chatRequest, "The request body can not be null.");

		return this.restClient.post()
				.uri("/v1/chat/completions")
				.body(chatRequest)
				.retrieve()
				.onStatus(this.responseErrorHandler)
				.body(ChatCompletion.class);
	}

	/**
	 * Creates a streaming chat response for the given chat conversation.
	 *
	 * @param chatRequest The chat completion request. Must have the stream property set to true.
	 * @return The chat completion Flux response.
	 */
	public Flux<ChatCompletionChunk> chatCompletionStreaming(
			ChatCompletionRequest chatRequest) {

		Assert.notNull(chatRequest, "The request body can not be null.");
		Assert.isTrue(chatRequest.stream(), "Request must set the steam property to true.");

		return webClient.post()
				.uri("/v1/chat/completions")
				.body(Mono.just(chatRequest), ChatCompletionRequest.class)
				.retrieve()
				// .onStatus(null, null)
				.bodyToFlux(String.class)
				.takeUntil(content -> content.contains("[DONE]"))
				.filter(content -> !content.contains("[DONE]"))
				.map(content -> {
					try {
						// System.out.println(content);
						return this.objectMapper.readValue(content, ChatCompletionChunk.class);
					}
					catch (JsonProcessingException e) {
						throw new RuntimeException(e);
					}
				});
	}
}
